{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "import tensorflow\r\n",
    "import gym\r\n",
    "import random\r\n",
    "import atari_py\r\n",
    "import numpy as np\r\n",
    "from tensorflow.keras.models import Sequential\r\n",
    "from tensorflow.keras.layers import Dense, Flatten, Convolution2D\r\n",
    "from tensorflow.keras.optimizers import Adam\r\n",
    "from rl.agents import DQNAgent\r\n",
    "from rl.memory import SequentialMemory\r\n",
    "from rl.policy import BoltzmannQPolicy, EpsGreedyQPolicy\r\n",
    "\r\n",
    "\r\n",
    "\r\n",
    "from gym import Env\r\n",
    "from gym.spaces import Discrete, Box\r\n",
    "import random\r\n",
    "\r\n",
    "import pandas as pd\r\n",
    "import tensorflow as tf\r\n",
    "import numpy as np\r\n",
    "\r\n",
    "from sklearn.model_selection import train_test_split\r\n",
    "from sklearn.preprocessing import LabelEncoder\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "source": [
    "all_df = pd.read_csv(\"train.csv\")\r\n",
    "all_df = all_df.dropna()\r\n",
    "\r\n",
    "\r\n",
    "\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "def room_to_numeric(cabin):\r\n",
    "    sum = 0\r\n",
    "    for i in range (0, len(cabin)  ):\r\n",
    "        sum += (ord(cabin[i]))\r\n",
    "    return sum"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "source": [],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "175"
      ]
     },
     "metadata": {},
     "execution_count": 47
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "source": [
    "for j in range (0,len(all_df)):\r\n",
    "    all_df.iloc[j,10] = room_to_numeric(all_df.iloc[j,10] )\r\n",
    "    if(all_df.iloc[j,4] == \"female\"):\r\n",
    "        all_df.iloc[j,4] = 1\r\n",
    "    else:\r\n",
    "        all_df.iloc[j,4] = 0\r\n",
    "\r\n",
    "\r\n",
    "all_df"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Name</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Ticket</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Cabin</th>\n",
       "      <th>Embarked</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Cumings, Mrs. John Bradley (Florence Briggs Th...</td>\n",
       "      <td>1</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>PC 17599</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>176</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Futrelle, Mrs. Jacques Heath (Lily May Peel)</td>\n",
       "      <td>1</td>\n",
       "      <td>35.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>113803</td>\n",
       "      <td>53.1000</td>\n",
       "      <td>217</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>McCarthy, Mr. Timothy J</td>\n",
       "      <td>0</td>\n",
       "      <td>54.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>17463</td>\n",
       "      <td>51.8625</td>\n",
       "      <td>175</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>Sandstrom, Miss. Marguerite Rut</td>\n",
       "      <td>1</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>PP 9549</td>\n",
       "      <td>16.7000</td>\n",
       "      <td>125</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Bonnell, Miss. Elizabeth</td>\n",
       "      <td>1</td>\n",
       "      <td>58.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>113783</td>\n",
       "      <td>26.5500</td>\n",
       "      <td>215</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>871</th>\n",
       "      <td>872</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Beckwith, Mrs. Richard Leonard (Sallie Monypeny)</td>\n",
       "      <td>1</td>\n",
       "      <td>47.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>11751</td>\n",
       "      <td>52.5542</td>\n",
       "      <td>172</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>872</th>\n",
       "      <td>873</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Carlsson, Mr. Frans Olof</td>\n",
       "      <td>0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>695</td>\n",
       "      <td>5.0000</td>\n",
       "      <td>574</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>879</th>\n",
       "      <td>880</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Potter, Mrs. Thomas Jr (Lily Alexenia Wilson)</td>\n",
       "      <td>1</td>\n",
       "      <td>56.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>11767</td>\n",
       "      <td>83.1583</td>\n",
       "      <td>168</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>887</th>\n",
       "      <td>888</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Graham, Miss. Margaret Edith</td>\n",
       "      <td>1</td>\n",
       "      <td>19.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>112053</td>\n",
       "      <td>30.0000</td>\n",
       "      <td>168</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>889</th>\n",
       "      <td>890</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Behr, Mr. Karl Howell</td>\n",
       "      <td>0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>111369</td>\n",
       "      <td>30.0000</td>\n",
       "      <td>224</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>183 rows Ã— 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     PassengerId  Survived  Pclass  \\\n",
       "1              2         1       1   \n",
       "3              4         1       1   \n",
       "6              7         0       1   \n",
       "10            11         1       3   \n",
       "11            12         1       1   \n",
       "..           ...       ...     ...   \n",
       "871          872         1       1   \n",
       "872          873         0       1   \n",
       "879          880         1       1   \n",
       "887          888         1       1   \n",
       "889          890         1       1   \n",
       "\n",
       "                                                  Name Sex   Age  SibSp  \\\n",
       "1    Cumings, Mrs. John Bradley (Florence Briggs Th...   1  38.0      1   \n",
       "3         Futrelle, Mrs. Jacques Heath (Lily May Peel)   1  35.0      1   \n",
       "6                              McCarthy, Mr. Timothy J   0  54.0      0   \n",
       "10                     Sandstrom, Miss. Marguerite Rut   1   4.0      1   \n",
       "11                            Bonnell, Miss. Elizabeth   1  58.0      0   \n",
       "..                                                 ...  ..   ...    ...   \n",
       "871   Beckwith, Mrs. Richard Leonard (Sallie Monypeny)   1  47.0      1   \n",
       "872                           Carlsson, Mr. Frans Olof   0  33.0      0   \n",
       "879      Potter, Mrs. Thomas Jr (Lily Alexenia Wilson)   1  56.0      0   \n",
       "887                       Graham, Miss. Margaret Edith   1  19.0      0   \n",
       "889                              Behr, Mr. Karl Howell   0  26.0      0   \n",
       "\n",
       "     Parch    Ticket     Fare Cabin Embarked  \n",
       "1        0  PC 17599  71.2833   176        C  \n",
       "3        0    113803  53.1000   217        S  \n",
       "6        0     17463  51.8625   175        S  \n",
       "10       1   PP 9549  16.7000   125        S  \n",
       "11       0    113783  26.5500   215        S  \n",
       "..     ...       ...      ...   ...      ...  \n",
       "871      1     11751  52.5542   172        S  \n",
       "872      0       695   5.0000   574        S  \n",
       "879      1     11767  83.1583   168        C  \n",
       "887      0    112053  30.0000   168        S  \n",
       "889      0    111369  30.0000   224        C  \n",
       "\n",
       "[183 rows x 12 columns]"
      ]
     },
     "metadata": {},
     "execution_count": 61
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "source": [
    "all_df = all_df.drop(all_df.columns[[0,3,8,11]], axis=1)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "source": [
    "all_df.iloc[1].tolist()[1:8]"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[1, 1, 35.0, 1, 0, 53.1, 217]"
      ]
     },
     "metadata": {},
     "execution_count": 78
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "source": [
    "class ShowerEnv(Env):\r\n",
    "    def __init__(self):\r\n",
    "        self.action_space = Discrete(2)\r\n",
    "        self.duration = 0\r\n",
    "        self.state = all_df.iloc[self.duration].tolist()[1:8]\r\n",
    "        \r\n",
    "        \r\n",
    "\r\n",
    "    def step(self, action):\r\n",
    "        self.duration +=1\r\n",
    "\r\n",
    "        self.state = all_df.iloc[self.duration].tolist()[1:8]\r\n",
    "        \r\n",
    "\r\n",
    "\r\n",
    "        if all_df.iloc[self.duration].tolist()[0] == self.action:\r\n",
    "            reward =1\r\n",
    "        else:\r\n",
    "            reward = -1\r\n",
    "\r\n",
    "        if self.duration >= 10000:\r\n",
    "            done = True\r\n",
    "        else:\r\n",
    "            done = False\r\n",
    "\r\n",
    "        info = {}\r\n",
    "\r\n",
    "        return self.state, reward, done, info\r\n",
    "\r\n",
    "    def render(self):\r\n",
    "        pass\r\n",
    "\r\n",
    "    def reset(self):\r\n",
    "        self.duration =0\r\n",
    "        self.state = all_df.iloc[self.duration].tolist()[1:8]\r\n",
    "        \r\n",
    "        return self.state\r\n",
    "\r\n",
    "    "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "source": [
    "env = ShowerEnv()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "source": [
    "env.action_space.sample()"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "metadata": {},
     "execution_count": 89
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "source": [
    "episodes = 10\r\n",
    "for episode in range(1, episodes+1):\r\n",
    "    state = env.reset()\r\n",
    "    done = False\r\n",
    "    score = 0\r\n",
    "    \r\n",
    "    while not done:\r\n",
    "        env.render()\r\n",
    "        action = env.action_space.sample()\r\n",
    "        n_state, reward, done, info = env.step(action)\r\n",
    "        score += reward\r\n",
    "    print('Episode:{} score:{}'.format(episode, score))\r\n",
    "env.close()"
   ],
   "outputs": [
    {
     "output_type": "error",
     "ename": "AttributeError",
     "evalue": "'ShowerEnv' object has no attribute 'action'",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-108-4f2bd0e52934>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      8\u001b[0m         \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m         \u001b[0maction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maction_space\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msample\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m         \u001b[0mn_state\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m         \u001b[0mscore\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Episode:{} score:{}'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepisode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscore\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-86-5a07338a2ed3>\u001b[0m in \u001b[0;36mstep\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m         \u001b[1;32mif\u001b[0m \u001b[0mall_df\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mduration\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m             \u001b[0mreward\u001b[0m \u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'ShowerEnv' object has no attribute 'action'"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "source": [
    "states = env.observation_space.shape\r\n",
    "print(states)\r\n",
    "actions = env.action_space.n"
   ],
   "outputs": [
    {
     "output_type": "error",
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'shape'",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-91-5e39bad08ab3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mstates\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mobservation_space\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstates\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mactions\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maction_space\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'shape'"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "source": [
    "def build_model():\r\n",
    "    model = Sequential()\r\n",
    "    \r\n",
    "    model.add(Dense(48, activation='relu', input_shape = [8,1]))\r\n",
    "    \r\n",
    "    model.add(Dense(21, activation = 'relu'))\r\n",
    "    model.add(Dense(2, activation = 'linear'))\r\n",
    "    return model\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "source": [
    "model = build_model()\r\n",
    "\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "source": [
    "def build_agent(model, actions):\r\n",
    "    policy = BoltzmannQPolicy()\r\n",
    "    \r\n",
    "    memory = SequentialMemory(limit =100, window_length =1)\r\n",
    "    \r\n",
    "    dqn = DQNAgent(model = model,\r\n",
    "                   memory=memory,\r\n",
    "                   policy=policy, \r\n",
    "                   \r\n",
    "                   nb_actions = actions,\r\n",
    "                   nb_steps_warmup =100,\r\n",
    "                   target_model_update=1e-2\r\n",
    "                  )\r\n",
    "    return dqn\r\n",
    "\r\n",
    "    \r\n",
    "    "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "source": [
    "dqn = build_agent(model, 2)\r\n",
    "dqn.compile(Adam(lr=1e-2), metrics=['mae'])\r\n",
    "dqn.fit(env, nb_steps = 100000, visualize=False, verbose =1)"
   ],
   "outputs": [
    {
     "output_type": "error",
     "ename": "ValueError",
     "evalue": "Model output \"Tensor(\"dense_7/BiasAdd:0\", shape=(None, 8, 2), dtype=float32)\" has invalid shape. DQN expects a model that has one dimension for each action, in this case 2.",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-107-090de2790d5d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdqn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbuild_agent\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mdqn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mAdam\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlr\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1e-2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'mae'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mdqn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnb_steps\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m100000\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvisualize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m \u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-106-f1eca1f8ded3>\u001b[0m in \u001b[0;36mbuild_agent\u001b[1;34m(model, actions)\u001b[0m\n\u001b[0;32m     10\u001b[0m                    \u001b[0mnb_actions\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mactions\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m                    \u001b[0mnb_steps_warmup\u001b[0m \u001b[1;33m=\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m                    \u001b[0mtarget_model_update\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1e-2\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m                   )\n\u001b[0;32m     14\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mdqn\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\games_rl\\lib\\site-packages\\rl\\agents\\dqn.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, model, policy, test_policy, enable_double_dqn, enable_dueling_network, dueling_type, *args, **kwargs)\u001b[0m\n\u001b[0;32m    105\u001b[0m         \u001b[1;31m# Validate (important) input.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    106\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnb_actions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 107\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf'Model output \"{model.output}\" has invalid shape. DQN expects a model that has one dimension for each action, in this case {self.nb_actions}.'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    108\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    109\u001b[0m         \u001b[1;31m# Parameters.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Model output \"Tensor(\"dense_7/BiasAdd:0\", shape=(None, 8, 2), dtype=float32)\" has invalid shape. DQN expects a model that has one dimension for each action, in this case 2."
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "source": [
    "scores = dqn.test(env, nb_episodes=100, visualize=False)\r\n",
    "print(np.mean(scores.history['episode_reward']))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Testing for 100 episodes ...\n",
      "Episode 1: reward: 30.000, steps: 60\n",
      "Episode 2: reward: 52.000, steps: 60\n",
      "Episode 3: reward: 56.000, steps: 60\n",
      "Episode 4: reward: 46.000, steps: 60\n",
      "Episode 5: reward: 20.000, steps: 60\n",
      "Episode 6: reward: 2.000, steps: 60\n",
      "Episode 7: reward: 34.000, steps: 60\n",
      "Episode 8: reward: 56.000, steps: 60\n",
      "Episode 9: reward: 60.000, steps: 60\n",
      "Episode 10: reward: -14.000, steps: 60\n",
      "Episode 11: reward: 58.000, steps: 60\n",
      "Episode 12: reward: 36.000, steps: 60\n",
      "Episode 13: reward: 6.000, steps: 60\n",
      "Episode 14: reward: 18.000, steps: 60\n",
      "Episode 15: reward: -6.000, steps: 60\n",
      "Episode 16: reward: 38.000, steps: 60\n",
      "Episode 17: reward: 30.000, steps: 60\n",
      "Episode 18: reward: -50.000, steps: 60\n",
      "Episode 19: reward: 16.000, steps: 60\n",
      "Episode 20: reward: 26.000, steps: 60\n",
      "Episode 21: reward: -10.000, steps: 60\n",
      "Episode 22: reward: -16.000, steps: 60\n",
      "Episode 23: reward: 2.000, steps: 60\n",
      "Episode 24: reward: 26.000, steps: 60\n",
      "Episode 25: reward: 60.000, steps: 60\n",
      "Episode 26: reward: -52.000, steps: 60\n",
      "Episode 27: reward: -48.000, steps: 60\n",
      "Episode 28: reward: 0.000, steps: 60\n",
      "Episode 29: reward: 40.000, steps: 60\n",
      "Episode 30: reward: 60.000, steps: 60\n",
      "Episode 31: reward: -10.000, steps: 60\n",
      "Episode 32: reward: 60.000, steps: 60\n",
      "Episode 33: reward: -30.000, steps: 60\n",
      "Episode 34: reward: -38.000, steps: 60\n",
      "Episode 35: reward: 22.000, steps: 60\n",
      "Episode 36: reward: -28.000, steps: 60\n",
      "Episode 37: reward: 58.000, steps: 60\n",
      "Episode 38: reward: 60.000, steps: 60\n",
      "Episode 39: reward: -26.000, steps: 60\n",
      "Episode 40: reward: 50.000, steps: 60\n",
      "Episode 41: reward: -22.000, steps: 60\n",
      "Episode 42: reward: -20.000, steps: 60\n",
      "Episode 43: reward: -2.000, steps: 60\n",
      "Episode 44: reward: -32.000, steps: 60\n",
      "Episode 45: reward: -50.000, steps: 60\n",
      "Episode 46: reward: 4.000, steps: 60\n",
      "Episode 47: reward: 52.000, steps: 60\n",
      "Episode 48: reward: 22.000, steps: 60\n",
      "Episode 49: reward: 8.000, steps: 60\n",
      "Episode 50: reward: 6.000, steps: 60\n",
      "Episode 51: reward: 60.000, steps: 60\n",
      "Episode 52: reward: 16.000, steps: 60\n",
      "Episode 53: reward: 28.000, steps: 60\n",
      "Episode 54: reward: 8.000, steps: 60\n",
      "Episode 55: reward: 60.000, steps: 60\n",
      "Episode 56: reward: 16.000, steps: 60\n",
      "Episode 57: reward: 0.000, steps: 60\n",
      "Episode 58: reward: 60.000, steps: 60\n",
      "Episode 59: reward: 26.000, steps: 60\n",
      "Episode 60: reward: 38.000, steps: 60\n",
      "Episode 61: reward: 34.000, steps: 60\n",
      "Episode 62: reward: -48.000, steps: 60\n",
      "Episode 63: reward: -2.000, steps: 60\n",
      "Episode 64: reward: 16.000, steps: 60\n",
      "Episode 65: reward: 60.000, steps: 60\n",
      "Episode 66: reward: -14.000, steps: 60\n",
      "Episode 67: reward: -12.000, steps: 60\n",
      "Episode 68: reward: -6.000, steps: 60\n",
      "Episode 69: reward: 60.000, steps: 60\n",
      "Episode 70: reward: 48.000, steps: 60\n",
      "Episode 71: reward: 50.000, steps: 60\n",
      "Episode 72: reward: 2.000, steps: 60\n",
      "Episode 73: reward: -30.000, steps: 60\n",
      "Episode 74: reward: 30.000, steps: 60\n",
      "Episode 75: reward: 30.000, steps: 60\n",
      "Episode 76: reward: 12.000, steps: 60\n",
      "Episode 77: reward: 30.000, steps: 60\n",
      "Episode 78: reward: -22.000, steps: 60\n",
      "Episode 79: reward: -38.000, steps: 60\n",
      "Episode 80: reward: 20.000, steps: 60\n",
      "Episode 81: reward: -28.000, steps: 60\n",
      "Episode 82: reward: 56.000, steps: 60\n",
      "Episode 83: reward: 40.000, steps: 60\n",
      "Episode 84: reward: 0.000, steps: 60\n",
      "Episode 85: reward: -14.000, steps: 60\n",
      "Episode 86: reward: 38.000, steps: 60\n",
      "Episode 87: reward: 28.000, steps: 60\n",
      "Episode 88: reward: -28.000, steps: 60\n",
      "Episode 89: reward: 60.000, steps: 60\n",
      "Episode 90: reward: 46.000, steps: 60\n",
      "Episode 91: reward: 60.000, steps: 60\n",
      "Episode 92: reward: 48.000, steps: 60\n",
      "Episode 93: reward: 46.000, steps: 60\n",
      "Episode 94: reward: 10.000, steps: 60\n",
      "Episode 95: reward: 26.000, steps: 60\n",
      "Episode 96: reward: 4.000, steps: 60\n",
      "Episode 97: reward: 24.000, steps: 60\n",
      "Episode 98: reward: -12.000, steps: 60\n",
      "Episode 99: reward: 32.000, steps: 60\n",
      "Episode 100: reward: 28.000, steps: 60\n",
      "16.56\n"
     ]
    }
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.7.11",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.11 64-bit ('games_rl': conda)"
  },
  "interpreter": {
   "hash": "e99ec45d6c852ea5fa3d4ad7a082220a83a169bae09a9553fb02d3b9b2eecb83"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}